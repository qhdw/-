初始化时，若采用零初始化，效果均能达到0.92左右，，采用0-1的浮点数初始化时准确率在0.91，采用正态分布的初始化时，准确率则在0.87左右，原因在于后两者训练速度较慢，训练未完成。

如果在神经网络中添加一层线性变换，也就是取消代码中的线性变换注释时，会发现出现nan值。原因在于多分类时有y的值为0，使得log(y)变为无穷大，导致无法训练。

其实添加这一层后无法训练的原因主要是两点，第一点在于输入数据没有归一化，第二点在于权重(w1)的初始化问题。

对于mnist数据集，只需要采用合适的权重初始化方法即可。对于大多数的初始化权重方法，需要在计算损失前添加梯度截断方法，虽然叫梯度截断，但在这里只用于y值截断，使得log(y)在一个可接受的范围，在这种情况下即可完成训练，但是可能训练效果较差。但是如果采用tf.truncated_normal()方法或xavier_init方法(未在TensorFlow中集成，需自行实现)可以得到一个非常好的结果，而且无需添加梯度截断，这表明了权重初始化在神经网络训练过程中的重要性，而且这也是深层神经网络难以训练的一个非常重要的原因，神经网络的预训练在某种意义上就是得到有效的权重初始化方法。